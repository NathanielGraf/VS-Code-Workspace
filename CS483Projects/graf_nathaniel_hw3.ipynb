{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d2f774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\natha\\AppData\\Local\\Temp\\ipykernel_16500\\844815121.py:19: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return float(cost)\n"
     ]
    }
   ],
   "source": [
    "#Question 1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    " \n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    \n",
    "    #Add epsilon to not log 0 \n",
    "    epsilon = 1e-10\n",
    "    \n",
    "    #Cost function, log likelihood\n",
    "    cost = (-1/m) * (y.T @ np.log(h + epsilon) + (1 - y).T @ np.log(1 - h + epsilon))\n",
    "    return float(cost)\n",
    "\n",
    "#Gradient descent\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        h = sigmoid(X @ theta)\n",
    "        \n",
    "        #Gradient is derivative of cost function\n",
    "        gradient = (1/m) * (X.T @ (h - y))\n",
    "        \n",
    "        #Update theta down the slope\n",
    "        theta = theta - alpha * gradient\n",
    "        \n",
    "        cost = compute_cost(X, y, theta)\n",
    "        cost_history.append(cost)\n",
    "    \n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "#Predict function\n",
    "def predict(X, theta):\n",
    "    predictions = sigmoid(X @ theta)\n",
    "    return (predictions >= 0.5).astype(int)\n",
    "\n",
    "#Calculate accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred) * 100\n",
    "\n",
    "#Make dataset \n",
    "\n",
    "np.random.seed(1)\n",
    "m = 100  \n",
    "\n",
    "#Make 2 groups around (-2, -2) and (2, 2)\n",
    "X_class0 = np.random.randn(m//2, 2) + np.array([-2, -2])\n",
    "y_class0 = np.zeros((m//2, 1))\n",
    "X_class1 = np.random.randn(m//2, 2) + np.array([2, 2])\n",
    "y_class1 = np.ones((m//2, 1))\n",
    "\n",
    "#Combine the data\n",
    "X = np.vstack([X_class0, X_class1])\n",
    "y = np.vstack([y_class0, y_class1])\n",
    "\n",
    "#Add column of ones so we can have an intercept\n",
    "X_with_bias = np.hstack([np.ones((m, 1)), X])\n",
    "\n",
    "\n",
    "theta = np.zeros((X_with_bias.shape[1], 1))\n",
    "alpha = 0.1\n",
    "iterations = 5000\n",
    "\n",
    "theta_final, cost_history = gradient_descent(X_with_bias, y, theta, alpha, iterations)\n",
    "\n",
    "#Make predictions\n",
    "y_pred = predict(X_with_bias, theta_final)\n",
    "\n",
    "#Calculate accuracy\n",
    "acc = accuracy(y, y_pred)\n",
    "print(f\"Model Accuracy: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68a094a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X: [[1, 1], [1, 2], [1, 3]]\n",
      "Input y: [1, 2, 3]\n",
      "Output coefficients: [0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def normal_equation(X, y):\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    \n",
    "    #Normal equation (use pseudoinverse) \n",
    "    #theta = (X^T * X)^-1 * X^T * y = pinv(X) * y\n",
    "    theta = np.linalg.pinv(X) @ y\n",
    "    \n",
    "    #Round\n",
    "    result = np.round(theta.flatten(), 4).tolist()\n",
    "    \n",
    "    return result\n",
    "\n",
    "#Test with example\n",
    "X = [[1, 1], [1, 2], [1, 3]]\n",
    "y = [1, 2, 3]\n",
    "\n",
    "coefficients = normal_equation(X, y)\n",
    "print(f\"Input X: {X}\")\n",
    "print(f\"Input y: {y}\")\n",
    "print(f\"Output coefficients: {coefficients}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b12103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X: [[1 1]\n",
      " [1 2]\n",
      " [1 3]]\n",
      "Input y: [1 2 3]\n",
      "Alpha: 0.01\n",
      "Iterations: 1000\n",
      "Output coefficients: [0.1107 0.9513]\n"
     ]
    }
   ],
   "source": [
    "#Question 3\n",
    "\n",
    "#Can't use the same one as q1 since it uses sigmoid \n",
    "def gradient_descent(X, y, alpha, iterations):\n",
    "    \n",
    "    m = len(y)\n",
    "    n = X.shape[1]\n",
    "    theta = np.zeros((n, 1))\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    #Gradient descent loop\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        predictions = X @ theta\n",
    "        \n",
    "        #Calculate error\n",
    "        errors = predictions - y\n",
    "        \n",
    "        #Update theta using gradient\n",
    "        gradient = (1/m) * (X.T @ errors)\n",
    "        theta = theta - alpha * gradient\n",
    "    \n",
    "    #Round\n",
    "    result = np.round(theta.flatten(), 4)\n",
    "    \n",
    "    return result\n",
    "\n",
    "#Test with example\n",
    "X = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "y = np.array([1, 2, 3])\n",
    "alpha = 0.01\n",
    "iterations = 1000\n",
    "\n",
    "coefficients = gradient_descent(X, y, alpha, iterations)\n",
    "print(f\"Input X: {X}\")\n",
    "print(f\"Input y: {y}\")\n",
    "print(f\"Alpha: {alpha}\")\n",
    "print(f\"Iterations: {iterations}\")\n",
    "print(f\"Output coefficients: {coefficients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74c925",
   "metadata": {},
   "source": [
    "Report: \n",
    "\n",
    "One thing I didn't realize coming in was how much easier it is to code for linear regression compared to logistic regression. The gradient descent we had to do for logreg with the cost function, although still simple, was a lot longer than using the normal equation or gradient descent for linreg. I think this stems from the cost function, in linreg you can just use the error as your metric of evaluation. The coding principles used in this are just basic functions, matrix operations using numpy, and implementation of the proper formulas for our models. Implementing was pretty easy, just had to follow the steps for gradient descent as described, and then do the simple matrix operations to iterate with the model. The lessons I learned were how to implement both linreg and logreg from scratch without using scikitlearn or something similar, as although I've used linreg many many times in the past, I had not made it from scratch yet. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
